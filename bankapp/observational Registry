observation registry use

Ah nice â€” youâ€™re stepping into Micrometer ObservationRegistry, which is basically the modern, structured, metrics + tracing-friendly replacement for ad-hoc timing logs ğŸ‘Œ

Iâ€™ll show what it is, why to use it, and how to wire it into your AOP.

What is ObservationRegistry?

ObservationRegistry is Micrometerâ€™s central place to record:

â± execution time

ğŸ“Š metrics (Prometheus, Datadog, etc.)

ğŸ”— traces (OpenTelemetry, Zipkin, Jaeger)

ğŸ§  contextual tags (method, class, error, etc.)

One observation =
logs + metrics + traces, all from one API.

When should you use it instead of logging?
Use case	Logging	Observation
Just print time	âœ…	âš ï¸ overkill
Metrics dashboard	âŒ	âœ…
Distributed tracing	âŒ	âœ…
Latency percentiles	âŒ	âœ…
Production monitoring	âŒ	âœ…

ğŸ‘‰ If this is production code, ObservationRegistry is the right move.

Dependency (Spring Boot)
<dependency>
    <groupId>io.micrometer</groupId>
    <artifactId>micrometer-observation</artifactId>
</dependency>


(If you already use spring-boot-starter-actuator, you probably have it.)

Inject ObservationRegistry

Spring auto-configures it for you.

@Autowired
private ObservationRegistry observationRegistry;


or constructor injection (recommended).

AOP with ObservationRegistry (ğŸ”¥ clean version)
@Aspect
@Component
public class PerformanceTrackerAspect {

    private final ObservationRegistry observationRegistry;

    public PerformanceTrackerAspect(ObservationRegistry observationRegistry) {
        this.observationRegistry = observationRegistry;
    }

    @Around("@annotation(PerformanceTracker)")
    public Object around(ProceedingJoinPoint point) throws Throwable {

        MethodSignature signature = (MethodSignature) point.getSignature();
        String methodName = signature.getDeclaringType().getSimpleName()
                + "." + signature.getMethod().getName();

        return Observation
            .createNotStarted("method.performance", observationRegistry)
            .lowCardinalityKeyValue("method", methodName)
            .observeChecked(() -> point.proceed());
    }
}

What this gives you automatically

execution time

success/failure status

exception propagation (unchanged)

integration with metrics + tracing

Add error awareness (recommended)
return Observation
    .createNotStarted("method.performance", observationRegistry)
    .lowCardinalityKeyValue("method", methodName)
    .observeChecked(() -> {
        try {
            return point.proceed();
        } catch (Throwable t) {
            Observation.currentObservation()
                .error(t);
            throw t;
        }
    });


Now failures are marked correctly in metrics & traces.

How it shows up (examples)
Prometheus metric
method_performance_seconds_count{method="OrderService.createOrder"} 1
method_performance_seconds_sum{method="OrderService.createOrder"} 0.143

Trace
method.performance
 â””â”€â”€ OrderService.createOrder

Why this is better than System.currentTimeMillis()

monotonic timing

percentile support (p95, p99)

trace correlation

zero vendor lock-in

structured, queryable data

Pro tip âš¡

Use low-cardinality tags only (method names = ok)
âŒ never put IDs, payloads, or user data in tags

TL;DR

ObservationRegistry = production-grade timing + tracing

AOP + Observation = clean & transparent

Exceptions still propagate normally

Metrics/traces/logs all benefit

If you want next:

class-level observations

custom observation names

OpenTelemetry export

combining logging + observation

Just tell me ğŸš€